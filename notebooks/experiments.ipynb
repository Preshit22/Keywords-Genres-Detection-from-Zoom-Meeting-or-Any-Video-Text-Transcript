{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import speech_recognition as sr \n",
    "import moviepy.editor as mp\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from pytube import YouTube\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### video to text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in ../data/segments_audio/converted1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in ../data/segments_audio/converted2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "RecognizedSpeech:\n",
      "Once upon a time in a small village their livelihood and Rohan Rohan was known for his laziness and lack of ambition he had a habit of wasting time and procrastinating on important tasks despite the constant advice from his family and friends to value of time to understand the importance of my life understanding the young man struggles symbolise this time\n",
      "I want you to imagine that each kind of sound opportunity every time realising the gravity of action but how can I change how can I value\n"
     ]
    }
   ],
   "source": [
    "num_seconds_video = 2 * 60  # Only consider the first two minutes\n",
    "l = list(range(0, num_seconds_video + 1, 60))\n",
    "\n",
    "diz = {}\n",
    "for i in range(len(l) - 1):\n",
    "    ffmpeg_extract_subclip(\"../data/input/Life_Story.mp4\", max(0, l[i] - 2 * (l[i] != 0)), l[i + 1], targetname=\"../data/segments_video/cut{}.mp4\".format(i + 1))\n",
    "    clip = mp.VideoFileClip(r\"../data/segments_video/cut{}.mp4\".format(i + 1)) \n",
    "    clip.audio.write_audiofile(r\"../data/segments_audio/converted{}.wav\".format(i + 1))\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(\"../data/segments_audio/converted{}.wav\".format(i + 1)) as source:\n",
    "        audio_file = r.record(source)\n",
    "    diz['chunk{}'.format(i + 1)] = r.recognize_google(audio_file)\n",
    "\n",
    "text = '\\n'.join([diz['chunk{}'.format(i + 1)] for i in range(len(diz))])\n",
    "\n",
    "with open('../data/transcriptions/recognized.txt', mode='w') as file:\n",
    "    file.write(\"RecognizedSpeech:\\n\")\n",
    "    file.write(text)\n",
    "\n",
    "with open('../data/transcriptions/recognized.txt', 'r') as file:\n",
    "    print(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_link = \"https://www.youtube.com/watch?v=96iaZxKRmKg\"\n",
    "\n",
    "yt = YouTube(video_link)\n",
    "yt_stream = yt.streams.filter(progressive=True, file_extension='mp4').first()\n",
    "downloaded_filename = yt_stream.default_filename\n",
    "output_directory = \"../data/input/\"\n",
    "yt_stream.download(output_path=output_directory, filename=downloaded_filename)\n",
    "\n",
    "file_path = output_directory + downloaded_filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keywords identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant keywords:\n",
      "rohan\n",
      "time\n",
      "value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(txt):\n",
    "    txt = txt.lower() \n",
    "    txt = re.sub(r\"<.*?>\", \" \", txt)  \n",
    "    txt = re.sub(r\"[^a-zA-Z]\", \" \", txt)  \n",
    "    txt = nltk.word_tokenize(txt)  \n",
    "    txt = [word for word in txt if word not in stopwords.words('english')]  \n",
    "    txt = [word for word in txt if len(word) >= 3]  \n",
    "    return \" \".join(txt)\n",
    "with open('../data/transcriptions/recognized.txt', 'r', encoding='utf-8') as file:\n",
    "    texts = [preprocess_text(file.read())]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_threshold = 0.2\n",
    "relevant_keywords = tfidf_df.columns[tfidf_df.max() > tfidf_threshold]\n",
    "\n",
    "with open('../data/transcriptions/keywords.txt', 'w') as file:\n",
    "    for keyword in relevant_keywords:\n",
    "        file.write(keyword + '\\n')\n",
    "\n",
    "print(\"Relevant keywords:\")\n",
    "with open('../data/transcriptions/keywords.txt', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genre prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted genre is: Divorce\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"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\"\n",
    "\n",
    "def identify_genre(keywords):\n",
    "    url = \"https://api.openai.com/v1/engines/davinci/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    data = {\"prompt\": f\"{', '.join(keywords)} categorize the genre for above keywords. give in one word genre:?\", \"max_tokens\": 100}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    choices = response.json().get(\"choices\", [])\n",
    "    if choices:\n",
    "        genre = choices[0][\"text\"].split()[0].strip()\n",
    "        return genre\n",
    "    else:\n",
    "        return \"Genre prediction not available.\"\n",
    "with open('../data/transcriptions/keywords.txt', 'r', encoding='utf-8') as file:\n",
    "    keywords = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "predicted_genre = identify_genre(keywords)\n",
    "print(f\"The predicted genre is: {predicted_genre}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_link: str) -> str:\n",
    "    \"\"\"Download a YouTube video and return its file path.\"\"\"\n",
    "    yt = YouTube(video_link)\n",
    "    yt_stream = yt.streams.filter(progressive=True, file_extension='mp4').first()\n",
    "    downloaded_filename = yt_stream.default_filename\n",
    "    output_directory = \"../data/input/\"\n",
    "    yt_stream.download(output_path=output_directory, filename=downloaded_filename)\n",
    "    file_path = output_directory + downloaded_filename\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_text(file_path: str, num_seconds_video: int) -> str:\n",
    "    l = list(range(0, num_seconds_video + 1, 60))\n",
    "    diz = {}\n",
    "    for i in range(len(l) - 1):\n",
    "        ffmpeg_extract_subclip(file_path, max(0, l[i] - 2 * (l[i] != 0)), l[i + 1], targetname=\"../data/segments_video/cut{}.mp4\".format(i + 1))\n",
    "        clip = mp.VideoFileClip(r\"../data/segments_video/cut{}.mp4\".format(i + 1)) \n",
    "        clip.audio.write_audiofile(r\"../data/segments_audio/converted{}.wav\".format(i + 1))\n",
    "        r = sr.Recognizer()\n",
    "        with sr.AudioFile(\"../data/segments_audio/converted{}.wav\".format(i + 1)) as source:\n",
    "            audio_file = r.record(source)\n",
    "        diz['chunk{}'.format(i + 1)] = r.recognize_google(audio_file)\n",
    "    text = '\\n'.join([diz['chunk{}'.format(i + 1)] for i in range(len(diz))])\n",
    "    with open('../data/transcriptions/recognized.txt', mode='w') as file:\n",
    "        file.write(\"RecognizedSpeech:\\n\")\n",
    "        file.write(text)\n",
    "    return \"Video processing completed. The transcriptions have been saved to '../data/transcriptions/recognized.txt'.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "MoviePy - Writing audio in ../data/segments_audio/converted1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Video processing completed. The transcriptions have been saved to '../data/transcriptions/recognized.txt'.\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/input/Life_Story.mp4'\n",
    "num = 60\n",
    "message = video_to_text(file_path, num)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords():\n",
    "    def process(txt):\n",
    "        txt = txt.lower() \n",
    "        txt = re.sub(r\"<.*?>\", \" \", txt)  \n",
    "        txt = re.sub(r\"[^a-zA-Z]\", \" \", txt)  \n",
    "        txt = nltk.word_tokenize(txt)  \n",
    "        txt = [word for word in txt if word not in stopwords.words('english')]  \n",
    "        txt = [word for word in txt if len(word) >= 3]  \n",
    "        return \" \".join(txt)\n",
    "\n",
    "    with open('../data/transcriptions/recognized.txt', 'r', encoding='utf-8') as file:\n",
    "        texts = [process(file.read())]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    tfidf_threshold = 0.2\n",
    "    relevant_keywords = tfidf_df.columns[tfidf_df.max() > tfidf_threshold]\n",
    "\n",
    "    with open('../data/transcriptions/keywords.txt', 'w') as file:\n",
    "        for keyword in relevant_keywords:\n",
    "            file.write(keyword + '\\n')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted genre is: None\n"
     ]
    }
   ],
   "source": [
    "keywords = keywords()\n",
    "print(f\"The predicted genre is: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted genre is: alternative\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def identify_genre():\n",
    "    API_KEY = \"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\"\n",
    "    url = \"https://api.openai.com/v1/engines/davinci/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    \n",
    "    # Read keywords from a fixed file path\n",
    "    with open('../data/transcriptions/keywords.txt', 'r', encoding='utf-8') as file:\n",
    "        keywords = [line.strip() for line in file if line.strip()]\n",
    "    \n",
    "    data = {\"prompt\": f\"{', '.join(keywords)} categorize the genre for above keywords. give in one word genre:?\", \"max_tokens\": 100}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    choices = response.json().get(\"choices\", [])\n",
    "    if choices:\n",
    "        genre = choices[0][\"text\"].split()[0].strip()\n",
    "        return genre\n",
    "    else:\n",
    "        return \"Genre prediction not available.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
